Data replication and consistency are important aspects of any system where the availability and reliability of systems services are important.Additional concerns such as scalability, performance, and fault tolerance play a role in deciding the best methods for data replication.Data may be replicated at either file or data block levels, each presenting a unique set of challenges.Considerations such as the network partition size, update frequency, replication granularity, number of users, security, reliability, and mobility all affect choices on the data replication policy.Data replication in distributed systems, such as a Peer-To-Peer systems, where nodes are unreliable, additional considerations for the dynamic nature of nodes and data replication must be considered \cite{DRP}. Plover \cite{PLF} is a low-overhead file replication scheme for P2P networks that makes copies of files among physically close nodes based on capacities.Decentralized P2P file sharing programs such as Limewire, Bittorrent, Kazaa, etc. replicate file data on end nodes, which may become corrupt or suffer from loss of availability.Replica files are copies of a master file in a file replication system.Files are replicated along data paths to reduce hot spots and improve efficiency.  A file replication algorithm that achieves high query efficiency at a low query cost for decentralized file sharing systems has been developed \cite{EAD}.Demand based file replication and consistency by means of a group of File Replication Servers (FRS) \cite{DBF} has been explored which push data to other FRS when a requesting node requests a file.PAST \cite{PAST} and CFS \cite{CFS}, a wide-area Cooperative File System, replicate files on nodes close to the owner of the file.  While PAST stores whole files, CFS replicates data at the block level to distribute the load and storage space among servers in the network.CFS also uses Chord \cite{chord}, a scalable P2P lookup protocol for Internet Applications, to maintain routing tables used to find blocks.Web applications often benefit from data replication of application data by low latency access and reduced network traffic.Edge service delivery of content such as Content Delivery Networks (CDNs) like Akami typically cache static pages at end nodes and deliver them to customers from physically closer nodes, reducing overall latency.GlobeDB \cite{GDB} automatically replicates Web application data.  It does not replicate web pages, but rather the underlying data in the web application database allowing for a Web application to be rendered at end nodes.Distributed object caches such as MTCache, DBCache, and Memcached, are key-value stores that can also store the results of database queries as the value of the query key and allow for data replication for Web applications. 